[rank0]:V0219 06:57:50.025000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] Graph break in user code at /home/user/run-basic.py:36
[rank0]:V0219 06:57:50.025000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] Reason: Unsupported: Unexpected type in sourceless builder torch.distributed.tensor._dtensor_spec.DTensorSpec
[rank0]:V0219 06:57:50.025000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] User code traceback:
[rank0]:V0219 06:57:50.025000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]   File "/home/user/run-basic.py", line 85, in forward
[rank0]:V0219 06:57:50.025000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]     x = F.relu(self.conv2(x))
[rank0]:V0219 06:57:50.025000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]   File "/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 554, in forward
[rank0]:V0219 06:57:50.025000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]     return self._conv_forward(input, self.weight, self.bias)
[rank0]:V0219 06:57:50.025000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]   File "/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
[rank0]:V0219 06:57:50.025000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]     return F.conv2d(
[rank0]:V0219 06:57:50.025000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]   File "/home/user/run-basic.py", line 36, in patched_conv2d
[rank0]:V0219 06:57:50.025000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]     mesh = input._spec.mesh
[rank0]:V0219 06:57:50.025000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] 
[rank1]:V0219 06:57:50.026000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] Graph break in user code at /home/user/run-basic.py:36
[rank1]:V0219 06:57:50.026000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] Reason: Unsupported: Unexpected type in sourceless builder torch.distributed.tensor._dtensor_spec.DTensorSpec
[rank1]:V0219 06:57:50.026000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] User code traceback:
[rank1]:V0219 06:57:50.026000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]   File "/home/user/run-basic.py", line 85, in forward
[rank1]:V0219 06:57:50.026000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]     x = F.relu(self.conv2(x))
[rank1]:V0219 06:57:50.026000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]   File "/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 554, in forward
[rank1]:V0219 06:57:50.026000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]     return self._conv_forward(input, self.weight, self.bias)
[rank1]:V0219 06:57:50.026000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]   File "/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
[rank1]:V0219 06:57:50.026000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]     return F.conv2d(
[rank1]:V0219 06:57:50.026000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]   File "/home/user/run-basic.py", line 36, in patched_conv2d
[rank1]:V0219 06:57:50.026000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks]     mesh = input._spec.mesh
[rank1]:V0219 06:57:50.026000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [0/0] [__graph_breaks] 
class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_conv1_parameters_weight_: "f32[16, 1, 2, 2]", L_self_modules_conv1_parameters_bias_: "f32[16]", L_x_: "f32[1, 1, 28, 28]"):
        l_self_modules_conv1_parameters_weight_ = L_self_modules_conv1_parameters_weight_
        l_self_modules_conv1_parameters_bias_ = L_self_modules_conv1_parameters_bias_
        l_x_ = L_x_
        
         # File: /home/user/run-basic.py:39 in patched_conv2d, code: weight = DTensor.from_local(weight, mesh, placements)
        weight: "f32[16, 1, 2, 2]" = torch__dynamo_variables_torch_prim_from_local(l_self_modules_conv1_parameters_weight_);  l_self_modules_conv1_parameters_weight_ = None
        
         # File: /home/user/run-basic.py:41 in patched_conv2d, code: bias = DTensor.from_local(bias, mesh, placements)
        bias: "f32[16]" = torch__dynamo_variables_torch_prim_from_local_1(l_self_modules_conv1_parameters_bias_);  l_self_modules_conv1_parameters_bias_ = None
        
         # File: /home/user/run-basic.py:42 in patched_conv2d, code: return _original_conv2d(input, weight, bias, stride, padding, dilation, groups)
        conv2d: "f32[1, 16, 14, 14]" = torch.conv2d(l_x_, weight, bias, (2, 2), (0, 0), (1, 1), 1);  l_x_ = weight = bias = None
        
         # File: /home/user/run-basic.py:84 in forward, code: x = F.relu(self.conv1(x))
        x: "f32[1, 16, 14, 14]" = torch.nn.functional.relu(conv2d);  conv2d = None
        return (x,)
        
opcode         name                                     target                                                                                                                          args                                             kwargs
-------------  ---------------------------------------  ------------------------------------------------------------------------------------------------------------------------------  -----------------------------------------------  --------
placeholder    l_self_modules_conv1_parameters_weight_  L_self_modules_conv1_parameters_weight_                                                                                         ()                                               {}
placeholder    l_self_modules_conv1_parameters_bias_    L_self_modules_conv1_parameters_bias_                                                                                           ()                                               {}
placeholder    l_x_                                     L_x_                                                                                                                            ()                                               {}
call_function  weight                                   <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7f7db0452c20>  (l_self_modules_conv1_parameters_weight_,)       {}
call_function  bias                                     <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7f7db05867a0>  (l_self_modules_conv1_parameters_bias_,)         {}
call_function  conv2d                                   <built-in method conv2d of type object at 0x7f7e8561fec0>                                                                       (l_x_, weight, bias, (2, 2), (0, 0), (1, 1), 1)  {}
call_function  x                                        <function relu at 0x7f7dc641f2e0>                                                                                               (conv2d,)                                        {}
output         output                                   output                                                                                                                          ((x,),)                                          {}
class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_conv1_parameters_weight_: "f32[16, 1, 2, 2]", L_self_modules_conv1_parameters_bias_: "f32[16]", L_x_: "f32[1, 1, 28, 28]"):
        l_self_modules_conv1_parameters_weight_ = L_self_modules_conv1_parameters_weight_
        l_self_modules_conv1_parameters_bias_ = L_self_modules_conv1_parameters_bias_
        l_x_ = L_x_
        
         # File: /home/user/run-basic.py:39 in patched_conv2d, code: weight = DTensor.from_local(weight, mesh, placements)
        weight: "f32[16, 1, 2, 2]" = torch__dynamo_variables_torch_prim_from_local(l_self_modules_conv1_parameters_weight_);  l_self_modules_conv1_parameters_weight_ = None
        
         # File: /home/user/run-basic.py:41 in patched_conv2d, code: bias = DTensor.from_local(bias, mesh, placements)
        bias: "f32[16]" = torch__dynamo_variables_torch_prim_from_local_1(l_self_modules_conv1_parameters_bias_);  l_self_modules_conv1_parameters_bias_ = None
        
         # File: /home/user/run-basic.py:42 in patched_conv2d, code: return _original_conv2d(input, weight, bias, stride, padding, dilation, groups)
        conv2d: "f32[1, 16, 14, 14]" = torch.conv2d(l_x_, weight, bias, (2, 2), (0, 0), (1, 1), 1);  l_x_ = weight = bias = None
        
         # File: /home/user/run-basic.py:84 in forward, code: x = F.relu(self.conv1(x))
        x: "f32[1, 16, 14, 14]" = torch.nn.functional.relu(conv2d);  conv2d = None
        return (x,)
        
opcode         name                                     target                                                                                                                          args                                             kwargs
-------------  ---------------------------------------  ------------------------------------------------------------------------------------------------------------------------------  -----------------------------------------------  --------
placeholder    l_self_modules_conv1_parameters_weight_  L_self_modules_conv1_parameters_weight_                                                                                         ()                                               {}
placeholder    l_self_modules_conv1_parameters_bias_    L_self_modules_conv1_parameters_bias_                                                                                           ()                                               {}
placeholder    l_x_                                     L_x_                                                                                                                            ()                                               {}
call_function  weight                                   <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7f27e4156c20>  (l_self_modules_conv1_parameters_weight_,)       {}
call_function  bias                                     <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7f27f416a7a0>  (l_self_modules_conv1_parameters_bias_,)         {}
call_function  conv2d                                   <built-in method conv2d of type object at 0x7f28c081fec0>                                                                       (l_x_, weight, bias, (2, 2), (0, 0), (1, 1), 1)  {}
call_function  x                                        <function relu at 0x7f28015272e0>                                                                                               (conv2d,)                                        {}
output         output                                   output                                                                                                                          ((x,),)                                          {}
class GraphModule(torch.nn.Module):
    def forward(self, L_input_: "f32[1, 16, 14, 14]", L_weight_: "f32[32, 16, 2, 2]", L_bias_: "f32[32]"):
        l_input_ = L_input_
        l_weight_ = L_weight_
        l_bias_ = L_bias_
        
         # File: /home/user/run-basic.py:39 in patched_conv2d, code: weight = DTensor.from_local(weight, mesh, placements)
        weight: "f32[32, 16, 2, 2]" = torch__dynamo_variables_torch_prim_from_local(l_weight_);  l_weight_ = None
        
         # File: /home/user/run-basic.py:41 in patched_conv2d, code: bias = DTensor.from_local(bias, mesh, placements)
        bias: "f32[32]" = torch__dynamo_variables_torch_prim_from_local_1(l_bias_);  l_bias_ = None
        
         # File: /home/user/run-basic.py:42 in patched_conv2d, code: return _original_conv2d(input, weight, bias, stride, padding, dilation, groups)
        conv2d: "f32[1, 32, 7, 7]" = torch.conv2d(l_input_, weight, bias, (2, 2), (0, 0), (1, 1), 1);  l_input_ = weight = bias = None
        return (conv2d,)
        
opcode         name       target                                                                                                                          args                                                 kwargs
-------------  ---------  ------------------------------------------------------------------------------------------------------------------------------  ---------------------------------------------------  --------
placeholder    l_input_   L_input_                                                                                                                        ()                                                   {}
placeholder    l_weight_  L_weight_                                                                                                                       ()                                                   {}
placeholder    l_bias_    L_bias_                                                                                                                         ()                                                   {}
call_function  weight     <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7f7dba607b50>  (l_weight_,)                                         {}
call_function  bias       <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7f7db02b6320>  (l_bias_,)                                           {}
call_function  conv2d     <built-in method conv2d of type object at 0x7f7e8561fec0>                                                                       (l_input_, weight, bias, (2, 2), (0, 0), (1, 1), 1)  {}
output         output     output                                                                                                                          ((conv2d,),)                                         {}
class GraphModule(torch.nn.Module):
    def forward(self, L_input_: "f32[1, 16, 14, 14]", L_weight_: "f32[32, 16, 2, 2]", L_bias_: "f32[32]"):
        l_input_ = L_input_
        l_weight_ = L_weight_
        l_bias_ = L_bias_
        
         # File: /home/user/run-basic.py:39 in patched_conv2d, code: weight = DTensor.from_local(weight, mesh, placements)
        weight: "f32[32, 16, 2, 2]" = torch__dynamo_variables_torch_prim_from_local(l_weight_);  l_weight_ = None
        
         # File: /home/user/run-basic.py:41 in patched_conv2d, code: bias = DTensor.from_local(bias, mesh, placements)
        bias: "f32[32]" = torch__dynamo_variables_torch_prim_from_local_1(l_bias_);  l_bias_ = None
        
         # File: /home/user/run-basic.py:42 in patched_conv2d, code: return _original_conv2d(input, weight, bias, stride, padding, dilation, groups)
        conv2d: "f32[1, 32, 7, 7]" = torch.conv2d(l_input_, weight, bias, (2, 2), (0, 0), (1, 1), 1);  l_input_ = weight = bias = None
        return (conv2d,)
        
opcode         name       target                                                                                                                          args                                                 kwargs
-------------  ---------  ------------------------------------------------------------------------------------------------------------------------------  ---------------------------------------------------  --------
placeholder    l_input_   L_input_                                                                                                                        ()                                                   {}
placeholder    l_weight_  L_weight_                                                                                                                       ()                                                   {}
placeholder    l_bias_    L_bias_                                                                                                                         ()                                                   {}
call_function  weight     <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7f27f570fb50>  (l_weight_,)                                         {}
call_function  bias       <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7f27c432e440>  (l_bias_,)                                           {}
call_function  conv2d     <built-in method conv2d of type object at 0x7f28c081fec0>                                                                       (l_input_, weight, bias, (2, 2), (0, 0), (1, 1), 1)  {}
output         output     output                                                                                                                          ((conv2d,),)                                         {}
[rank0]:V0219 06:57:50.327000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] Graph break in user code at /home/user/run-basic.py:53
[rank0]:V0219 06:57:50.327000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] Reason: Unsupported: Unexpected type in sourceless builder torch.distributed.tensor._dtensor_spec.DTensorSpec
[rank0]:V0219 06:57:50.327000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] User code traceback:
[rank0]:V0219 06:57:50.327000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks]   File "/home/user/run-basic.py", line 87, in torch_dynamo_resume_in_forward_at_85
[rank0]:V0219 06:57:50.327000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks]     x = self.fc(x)
[rank0]:V0219 06:57:50.327000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks]   File "/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank0]:V0219 06:57:50.327000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks]     return F.linear(input, self.weight, self.bias)
[rank0]:V0219 06:57:50.327000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks]   File "/home/user/run-basic.py", line 53, in patched_linear
[rank0]:V0219 06:57:50.327000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks]     mesh = input._spec.mesh
[rank0]:V0219 06:57:50.327000 2602 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] 
class GraphModule(torch.nn.Module):
    def forward(self, L_stack1_: "f32[1, 32, 7, 7]"):
        l_stack1_ = L_stack1_
        
         # File: /home/user/run-basic.py:85 in torch_dynamo_resume_in_forward_at_85, code: x = F.relu(self.conv2(x))
        x: "f32[1, 32, 7, 7]" = torch.nn.functional.relu(l_stack1_);  l_stack1_ = None
        
         # File: /home/user/run-basic.py:86 in torch_dynamo_resume_in_forward_at_85, code: x = torch.flatten(x, 1)
        x_1: "f32[1, 1568]" = torch.flatten(x, 1);  x = None
        return (x_1,)
        
opcode         name       target                                                      args          kwargs
-------------  ---------  ----------------------------------------------------------  ------------  --------
placeholder    l_stack1_  L_stack1_                                                   ()            {}
call_function  x          <function relu at 0x7f7dc641f2e0>                           (l_stack1_,)  {}
call_function  x_1        <built-in method flatten of type object at 0x7f7e8561fec0>  (x, 1)        {}
output         output     output                                                      ((x_1,),)     {}
[rank1]:V0219 06:57:50.341000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] Graph break in user code at /home/user/run-basic.py:53
[rank1]:V0219 06:57:50.341000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] Reason: Unsupported: Unexpected type in sourceless builder torch.distributed.tensor._dtensor_spec.DTensorSpec
[rank1]:V0219 06:57:50.341000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] User code traceback:
[rank1]:V0219 06:57:50.341000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks]   File "/home/user/run-basic.py", line 87, in torch_dynamo_resume_in_forward_at_85
[rank1]:V0219 06:57:50.341000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks]     x = self.fc(x)
[rank1]:V0219 06:57:50.341000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks]   File "/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank1]:V0219 06:57:50.341000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks]     return F.linear(input, self.weight, self.bias)
[rank1]:V0219 06:57:50.341000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks]   File "/home/user/run-basic.py", line 53, in patched_linear
[rank1]:V0219 06:57:50.341000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks]     mesh = input._spec.mesh
[rank1]:V0219 06:57:50.341000 2603 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:435] [2/0] [__graph_breaks] 
class GraphModule(torch.nn.Module):
    def forward(self, L_stack1_: "f32[1, 32, 7, 7]"):
        l_stack1_ = L_stack1_
        
         # File: /home/user/run-basic.py:85 in torch_dynamo_resume_in_forward_at_85, code: x = F.relu(self.conv2(x))
        x: "f32[1, 32, 7, 7]" = torch.nn.functional.relu(l_stack1_);  l_stack1_ = None
        
         # File: /home/user/run-basic.py:86 in torch_dynamo_resume_in_forward_at_85, code: x = torch.flatten(x, 1)
        x_1: "f32[1, 1568]" = torch.flatten(x, 1);  x = None
        return (x_1,)
        
opcode         name       target                                                      args          kwargs
-------------  ---------  ----------------------------------------------------------  ------------  --------
placeholder    l_stack1_  L_stack1_                                                   ()            {}
call_function  x          <function relu at 0x7f28015272e0>                           (l_stack1_,)  {}
call_function  x_1        <built-in method flatten of type object at 0x7f28c081fec0>  (x, 1)        {}
output         output     output                                                      ((x_1,),)     {}
class GraphModule(torch.nn.Module):
    def forward(self, L_input_: "f32[1, 1568]", L_weight_: "f32[10, 1568]", L_bias_: "f32[10]"):
        l_input_ = L_input_
        l_weight_ = L_weight_
        l_bias_ = L_bias_
        
         # File: /home/user/run-basic.py:56 in patched_linear, code: weight = DTensor.from_local(weight, mesh, placements)
        weight: "f32[10, 1568]" = torch__dynamo_variables_torch_prim_from_local(l_weight_);  l_weight_ = None
        
         # File: /home/user/run-basic.py:58 in patched_linear, code: bias = DTensor.from_local(bias, mesh, placements)
        bias: "f32[10]" = torch__dynamo_variables_torch_prim_from_local_1(l_bias_);  l_bias_ = None
        
         # File: /home/user/run-basic.py:59 in patched_linear, code: return _original_linear(input, weight, bias)
        linear: "f32[1, 10]" = torch._C._nn.linear(l_input_, weight, bias);  l_input_ = weight = bias = None
        return (linear,)
        
opcode         name       target                                                                                                                          args                      kwargs
-------------  ---------  ------------------------------------------------------------------------------------------------------------------------------  ------------------------  --------
placeholder    l_input_   L_input_                                                                                                                        ()                        {}
placeholder    l_weight_  L_weight_                                                                                                                       ()                        {}
placeholder    l_bias_    L_bias_                                                                                                                         ()                        {}
call_function  weight     <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7f7db0206b90>  (l_weight_,)              {}
call_function  bias       <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7f7db0164d30>  (l_bias_,)                {}
call_function  linear     <built-in function linear>                                                                                                      (l_input_, weight, bias)  {}
output         output     output                                                                                                                          ((linear,),)              {}
class GraphModule(torch.nn.Module):
    def forward(self, L_input_: "f32[1, 1568]", L_weight_: "f32[10, 1568]", L_bias_: "f32[10]"):
        l_input_ = L_input_
        l_weight_ = L_weight_
        l_bias_ = L_bias_
        
         # File: /home/user/run-basic.py:56 in patched_linear, code: weight = DTensor.from_local(weight, mesh, placements)
        weight: "f32[10, 1568]" = torch__dynamo_variables_torch_prim_from_local(l_weight_);  l_weight_ = None
        
         # File: /home/user/run-basic.py:58 in patched_linear, code: bias = DTensor.from_local(bias, mesh, placements)
        bias: "f32[10]" = torch__dynamo_variables_torch_prim_from_local_1(l_bias_);  l_bias_ = None
        
         # File: /home/user/run-basic.py:59 in patched_linear, code: return _original_linear(input, weight, bias)
        linear: "f32[1, 10]" = torch._C._nn.linear(l_input_, weight, bias);  l_input_ = weight = bias = None
        return (linear,)
        
opcode         name       target                                                                                                                          args                      kwargs
-------------  ---------  ------------------------------------------------------------------------------------------------------------------------------  ------------------------  --------
placeholder    l_input_   L_input_                                                                                                                        ()                        {}
placeholder    l_weight_  L_weight_                                                                                                                       ()                        {}
placeholder    l_bias_    L_bias_                                                                                                                         ()                        {}
call_function  weight     <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7f27c427eb90>  (l_weight_,)              {}
call_function  bias       <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7f27c41d8d30>  (l_bias_,)                {}
call_function  linear     <built-in function linear>                                                                                                      (l_input_, weight, bias)  {}
output         output     output                                                                                                                          ((linear,),)              {}
class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "f32[1, 10]"):
        l_stack0_ = L_stack0_
        
         # File: /home/user/run-basic.py:88 in torch_dynamo_resume_in_forward_at_87, code: return F.log_softmax(x, dim=1)
        log_softmax: "f32[1, 10]" = torch.nn.functional.log_softmax(l_stack0_, dim = 1);  l_stack0_ = None
        return (log_softmax,)
        
opcode         name         target                                    args               kwargs
-------------  -----------  ----------------------------------------  -----------------  ----------
placeholder    l_stack0_    L_stack0_                                 ()                 {}
call_function  log_softmax  <function log_softmax at 0x7f7dc641fc70>  (l_stack0_,)       {'dim': 1}
output         output       output                                    ((log_softmax,),)  {}
class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "f32[1, 10]"):
        l_stack0_ = L_stack0_
        
         # File: /home/user/run-basic.py:88 in torch_dynamo_resume_in_forward_at_87, code: return F.log_softmax(x, dim=1)
        log_softmax: "f32[1, 10]" = torch.nn.functional.log_softmax(l_stack0_, dim = 1);  l_stack0_ = None
        return (log_softmax,)
        
opcode         name         target                                    args               kwargs
-------------  -----------  ----------------------------------------  -----------------  ----------
placeholder    l_stack0_    L_stack0_                                 ()                 {}
call_function  log_softmax  <function log_softmax at 0x7f2801527c70>  (l_stack0_,)       {'dim': 1}
output         output       output                                    ((log_softmax,),)  {}