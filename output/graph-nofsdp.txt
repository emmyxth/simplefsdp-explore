torch.cuda.device_count(): 2
class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_conv1_parameters_weight_: "f32[16, 1, 2, 2]", L_self_modules_conv1_parameters_bias_: "f32[16]", L_x_: "f32[1, 1, 28, 28]"):
        l_self_modules_conv1_parameters_weight_ = L_self_modules_conv1_parameters_weight_
        l_self_modules_conv1_parameters_bias_ = L_self_modules_conv1_parameters_bias_
        l_x_ = L_x_
        
         # File: /home/user/run-basic.py:39 in patched_conv2d, code: weight = DTensor.from_local(weight, mesh, placements)
        weight: "f32[16, 1, 2, 2]" = torch__dynamo_variables_torch_prim_from_local(l_self_modules_conv1_parameters_weight_);  l_self_modules_conv1_parameters_weight_ = None
        
         # File: /home/user/run-basic.py:41 in patched_conv2d, code: bias = DTensor.from_local(bias, mesh, placements)
        bias: "f32[16]" = torch__dynamo_variables_torch_prim_from_local_1(l_self_modules_conv1_parameters_bias_);  l_self_modules_conv1_parameters_bias_ = None
        
         # File: /home/user/run-basic.py:42 in patched_conv2d, code: return _original_conv2d(input, weight, bias, stride, padding, dilation, groups)
        conv2d: "f32[1, 16, 14, 14]" = torch.conv2d(l_x_, weight, bias, (2, 2), (0, 0), (1, 1), 1);  l_x_ = weight = bias = None
        
         # File: /home/user/run-basic.py:84 in forward, code: x = F.relu(self.conv1(x))
        x: "f32[1, 16, 14, 14]" = torch.nn.functional.relu(conv2d);  conv2d = None
        return (x,)
        
opcode         name                                     target                                                                                                                          args                                             kwargs
-------------  ---------------------------------------  ------------------------------------------------------------------------------------------------------------------------------  -----------------------------------------------  --------
placeholder    l_self_modules_conv1_parameters_weight_  L_self_modules_conv1_parameters_weight_                                                                                         ()                                               {}
placeholder    l_self_modules_conv1_parameters_bias_    L_self_modules_conv1_parameters_bias_                                                                                           ()                                               {}
placeholder    l_x_                                     L_x_                                                                                                                            ()                                               {}
call_function  weight                                   <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7fece415ec20>  (l_self_modules_conv1_parameters_weight_,)       {}
call_function  bias                                     <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7fece42927a0>  (l_self_modules_conv1_parameters_bias_,)         {}
call_function  conv2d                                   <built-in method conv2d of type object at 0x7fedb941fec0>                                                                       (l_x_, weight, bias, (2, 2), (0, 0), (1, 1), 1)  {}
call_function  x                                        <function relu at 0x7fecfa1272e0>                                                                                               (conv2d,)                                        {}
output         output                                   output                                                                                                                          ((x,),)                                          {}
class GraphModule(torch.nn.Module):
    def forward(self, L_self_modules_conv1_parameters_weight_: "f32[16, 1, 2, 2]", L_self_modules_conv1_parameters_bias_: "f32[16]", L_x_: "f32[1, 1, 28, 28]"):
        l_self_modules_conv1_parameters_weight_ = L_self_modules_conv1_parameters_weight_
        l_self_modules_conv1_parameters_bias_ = L_self_modules_conv1_parameters_bias_
        l_x_ = L_x_
        
         # File: /home/user/run-basic.py:39 in patched_conv2d, code: weight = DTensor.from_local(weight, mesh, placements)
        weight: "f32[16, 1, 2, 2]" = torch__dynamo_variables_torch_prim_from_local(l_self_modules_conv1_parameters_weight_);  l_self_modules_conv1_parameters_weight_ = None
        
         # File: /home/user/run-basic.py:41 in patched_conv2d, code: bias = DTensor.from_local(bias, mesh, placements)
        bias: "f32[16]" = torch__dynamo_variables_torch_prim_from_local_1(l_self_modules_conv1_parameters_bias_);  l_self_modules_conv1_parameters_bias_ = None
        
         # File: /home/user/run-basic.py:42 in patched_conv2d, code: return _original_conv2d(input, weight, bias, stride, padding, dilation, groups)
        conv2d: "f32[1, 16, 14, 14]" = torch.conv2d(l_x_, weight, bias, (2, 2), (0, 0), (1, 1), 1);  l_x_ = weight = bias = None
        
         # File: /home/user/run-basic.py:84 in forward, code: x = F.relu(self.conv1(x))
        x: "f32[1, 16, 14, 14]" = torch.nn.functional.relu(conv2d);  conv2d = None
        return (x,)
        
opcode         name                                     target                                                                                                                          args                                             kwargs
-------------  ---------------------------------------  ------------------------------------------------------------------------------------------------------------------------------  -----------------------------------------------  --------
placeholder    l_self_modules_conv1_parameters_weight_  L_self_modules_conv1_parameters_weight_                                                                                         ()                                               {}
placeholder    l_self_modules_conv1_parameters_bias_    L_self_modules_conv1_parameters_bias_                                                                                           ()                                               {}
placeholder    l_x_                                     L_x_                                                                                                                            ()                                               {}
call_function  weight                                   <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7f25602f6c20>  (l_self_modules_conv1_parameters_weight_,)       {}
call_function  bias                                     <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7f256042e7a0>  (l_self_modules_conv1_parameters_bias_,)         {}
call_function  conv2d                                   <built-in method conv2d of type object at 0x7f2637a1fec0>                                                                       (l_x_, weight, bias, (2, 2), (0, 0), (1, 1), 1)  {}
call_function  x                                        <function relu at 0x7f257882b2e0>                                                                                               (conv2d,)                                        {}
output         output                                   output                                                                                                                          ((x,),)                                          {}
class GraphModule(torch.nn.Module):
    def forward(self, L_input_: "f32[1, 16, 14, 14]", L_weight_: "f32[32, 16, 2, 2]", L_bias_: "f32[32]"):
        l_input_ = L_input_
        l_weight_ = L_weight_
        l_bias_ = L_bias_
        
         # File: /home/user/run-basic.py:39 in patched_conv2d, code: weight = DTensor.from_local(weight, mesh, placements)
        weight: "f32[32, 16, 2, 2]" = torch__dynamo_variables_torch_prim_from_local(l_weight_);  l_weight_ = None
        
         # File: /home/user/run-basic.py:41 in patched_conv2d, code: bias = DTensor.from_local(bias, mesh, placements)
        bias: "f32[32]" = torch__dynamo_variables_torch_prim_from_local_1(l_bias_);  l_bias_ = None
        
         # File: /home/user/run-basic.py:42 in patched_conv2d, code: return _original_conv2d(input, weight, bias, stride, padding, dilation, groups)
        conv2d: "f32[1, 32, 7, 7]" = torch.conv2d(l_input_, weight, bias, (2, 2), (0, 0), (1, 1), 1);  l_input_ = weight = bias = None
        return (conv2d,)
        
opcode         name       target                                                                                                                          args                                                 kwargs
-------------  ---------  ------------------------------------------------------------------------------------------------------------------------------  ---------------------------------------------------  --------
placeholder    l_input_   L_input_                                                                                                                        ()                                                   {}
placeholder    l_weight_  L_weight_                                                                                                                       ()                                                   {}
placeholder    l_bias_    L_bias_                                                                                                                         ()                                                   {}
call_function  weight     <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7fecee30bb50>  (l_weight_,)                                         {}
call_function  bias       <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7fecd82ba050>  (l_bias_,)                                           {}
call_function  conv2d     <built-in method conv2d of type object at 0x7fedb941fec0>                                                                       (l_input_, weight, bias, (2, 2), (0, 0), (1, 1), 1)  {}
output         output     output                                                                                                                          ((conv2d,),)                                         {}
class GraphModule(torch.nn.Module):
    def forward(self, L_input_: "f32[1, 16, 14, 14]", L_weight_: "f32[32, 16, 2, 2]", L_bias_: "f32[32]"):
        l_input_ = L_input_
        l_weight_ = L_weight_
        l_bias_ = L_bias_
        
         # File: /home/user/run-basic.py:39 in patched_conv2d, code: weight = DTensor.from_local(weight, mesh, placements)
        weight: "f32[32, 16, 2, 2]" = torch__dynamo_variables_torch_prim_from_local(l_weight_);  l_weight_ = None
        
         # File: /home/user/run-basic.py:41 in patched_conv2d, code: bias = DTensor.from_local(bias, mesh, placements)
        bias: "f32[32]" = torch__dynamo_variables_torch_prim_from_local_1(l_bias_);  l_bias_ = None
        
         # File: /home/user/run-basic.py:42 in patched_conv2d, code: return _original_conv2d(input, weight, bias, stride, padding, dilation, groups)
        conv2d: "f32[1, 32, 7, 7]" = torch.conv2d(l_input_, weight, bias, (2, 2), (0, 0), (1, 1), 1);  l_input_ = weight = bias = None
        return (conv2d,)
        
opcode         name       target                                                                                                                          args                                                 kwargs
-------------  ---------  ------------------------------------------------------------------------------------------------------------------------------  ---------------------------------------------------  --------
placeholder    l_input_   L_input_                                                                                                                        ()                                                   {}
placeholder    l_weight_  L_weight_                                                                                                                       ()                                                   {}
placeholder    l_bias_    L_bias_                                                                                                                         ()                                                   {}
call_function  weight     <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7f256ca13b50>  (l_weight_,)                                         {}
call_function  bias       <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7f2560152200>  (l_bias_,)                                           {}
call_function  conv2d     <built-in method conv2d of type object at 0x7f2637a1fec0>                                                                       (l_input_, weight, bias, (2, 2), (0, 0), (1, 1), 1)  {}
output         output     output                                                                                                                          ((conv2d,),)                                         {}
class GraphModule(torch.nn.Module):
    def forward(self, L_stack1_: "f32[1, 32, 7, 7]"):
        l_stack1_ = L_stack1_
        
         # File: /home/user/run-basic.py:85 in torch_dynamo_resume_in_forward_at_85, code: x = F.relu(self.conv2(x))
        x: "f32[1, 32, 7, 7]" = torch.nn.functional.relu(l_stack1_);  l_stack1_ = None
        
         # File: /home/user/run-basic.py:86 in torch_dynamo_resume_in_forward_at_85, code: x = torch.flatten(x, 1)
        x_1: "f32[1, 1568]" = torch.flatten(x, 1);  x = None
        return (x_1,)
        
opcode         name       target                                                      args          kwargs
-------------  ---------  ----------------------------------------------------------  ------------  --------
placeholder    l_stack1_  L_stack1_                                                   ()            {}
call_function  x          <function relu at 0x7fecfa1272e0>                           (l_stack1_,)  {}
call_function  x_1        <built-in method flatten of type object at 0x7fedb941fec0>  (x, 1)        {}
output         output     output                                                      ((x_1,),)     {}
class GraphModule(torch.nn.Module):
    def forward(self, L_stack1_: "f32[1, 32, 7, 7]"):
        l_stack1_ = L_stack1_
        
         # File: /home/user/run-basic.py:85 in torch_dynamo_resume_in_forward_at_85, code: x = F.relu(self.conv2(x))
        x: "f32[1, 32, 7, 7]" = torch.nn.functional.relu(l_stack1_);  l_stack1_ = None
        
         # File: /home/user/run-basic.py:86 in torch_dynamo_resume_in_forward_at_85, code: x = torch.flatten(x, 1)
        x_1: "f32[1, 1568]" = torch.flatten(x, 1);  x = None
        return (x_1,)
        
opcode         name       target                                                      args          kwargs
-------------  ---------  ----------------------------------------------------------  ------------  --------
placeholder    l_stack1_  L_stack1_                                                   ()            {}
call_function  x          <function relu at 0x7f257882b2e0>                           (l_stack1_,)  {}
call_function  x_1        <built-in method flatten of type object at 0x7f2637a1fec0>  (x, 1)        {}
output         output     output                                                      ((x_1,),)     {}
class GraphModule(torch.nn.Module):
    def forward(self, L_input_: "f32[1, 1568]", L_weight_: "f32[10, 1568]", L_bias_: "f32[10]"):
        l_input_ = L_input_
        l_weight_ = L_weight_
        l_bias_ = L_bias_
        
         # File: /home/user/run-basic.py:56 in patched_linear, code: weight = DTensor.from_local(weight, mesh, placements)
        weight: "f32[10, 1568]" = torch__dynamo_variables_torch_prim_from_local(l_weight_);  l_weight_ = None
        
         # File: /home/user/run-basic.py:58 in patched_linear, code: bias = DTensor.from_local(bias, mesh, placements)
        bias: "f32[10]" = torch__dynamo_variables_torch_prim_from_local_1(l_bias_);  l_bias_ = None
        
         # File: /home/user/run-basic.py:59 in patched_linear, code: return _original_linear(input, weight, bias)
        linear: "f32[1, 10]" = torch._C._nn.linear(l_input_, weight, bias);  l_input_ = weight = bias = None
        return (linear,)
        
opcode         name       target                                                                                                                          args                      kwargs
-------------  ---------  ------------------------------------------------------------------------------------------------------------------------------  ------------------------  --------
placeholder    l_input_   L_input_                                                                                                                        ()                        {}
placeholder    l_weight_  L_weight_                                                                                                                       ()                        {}
placeholder    l_bias_    L_bias_                                                                                                                         ()                        {}
call_function  weight     <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7fecd820ab90>  (l_weight_,)              {}
call_function  bias       <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7fecd816cd30>  (l_bias_,)                {}
call_function  linear     <built-in function linear>                                                                                                      (l_input_, weight, bias)  {}
output         output     output                                                                                                                          ((linear,),)              {}
class GraphModule(torch.nn.Module):
    def forward(self, L_input_: "f32[1, 1568]", L_weight_: "f32[10, 1568]", L_bias_: "f32[10]"):
        l_input_ = L_input_
        l_weight_ = L_weight_
        l_bias_ = L_bias_
        
         # File: /home/user/run-basic.py:56 in patched_linear, code: weight = DTensor.from_local(weight, mesh, placements)
        weight: "f32[10, 1568]" = torch__dynamo_variables_torch_prim_from_local(l_weight_);  l_weight_ = None
        
         # File: /home/user/run-basic.py:58 in patched_linear, code: bias = DTensor.from_local(bias, mesh, placements)
        bias: "f32[10]" = torch__dynamo_variables_torch_prim_from_local_1(l_bias_);  l_bias_ = None
        
         # File: /home/user/run-basic.py:59 in patched_linear, code: return _original_linear(input, weight, bias)
        linear: "f32[1, 10]" = torch._C._nn.linear(l_input_, weight, bias);  l_input_ = weight = bias = None
        return (linear,)
        
opcode         name       target                                                                                                                          args                      kwargs
-------------  ---------  ------------------------------------------------------------------------------------------------------------------------------  ------------------------  --------
placeholder    l_input_   L_input_                                                                                                                        ()                        {}
placeholder    l_weight_  L_weight_                                                                                                                       ()                        {}
placeholder    l_bias_    L_bias_                                                                                                                         ()                        {}
call_function  weight     <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7f25600a2b90>  (l_weight_,)              {}
call_function  bias       <function TorchInGraphFunctionVariable._get_handlers.<locals>.handle_from_local.<locals>.fn_with_prim_types at 0x7f254c178d30>  (l_bias_,)                {}
call_function  linear     <built-in function linear>                                                                                                      (l_input_, weight, bias)  {}
output         output     output                                                                                                                          ((linear,),)              {}
class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "f32[1, 10]"):
        l_stack0_ = L_stack0_
        
         # File: /home/user/run-basic.py:88 in torch_dynamo_resume_in_forward_at_87, code: return F.log_softmax(x, dim=1)
        log_softmax: "f32[1, 10]" = torch.nn.functional.log_softmax(l_stack0_, dim = 1);  l_stack0_ = None
        return (log_softmax,)
        
opcode         name         target                                    args               kwargs
-------------  -----------  ----------------------------------------  -----------------  ----------
placeholder    l_stack0_    L_stack0_                                 ()                 {}
call_function  log_softmax  <function log_softmax at 0x7fecfa127c70>  (l_stack0_,)       {'dim': 1}
output         output       output                                    ((log_softmax,),)  {}
class GraphModule(torch.nn.Module):
    def forward(self, L_stack0_: "f32[1, 10]"):
        l_stack0_ = L_stack0_
        
         # File: /home/user/run-basic.py:88 in torch_dynamo_resume_in_forward_at_87, code: return F.log_softmax(x, dim=1)
        log_softmax: "f32[1, 10]" = torch.nn.functional.log_softmax(l_stack0_, dim = 1);  l_stack0_ = None
        return (log_softmax,)
        
opcode         name         target                                    args               kwargs
-------------  -----------  ----------------------------------------  -----------------  ----------
placeholder    l_stack0_    L_stack0_                                 ()                 {}
call_function  log_softmax  <function log_softmax at 0x7f257882bc70>  (l_stack0_,)       {'dim': 1}
output         output       output                                    ((log_softmax,),)  {}